```{r}


# The aim is to plot "Andrew Ng" like learning curves to assess a need of greater dataset by 
# analyzing "bias and variance"" of the model 

##################################################################
#
#
#
##################################################################




```

```{r}
# DATA PREPARATION

# Remove rows from data1 with NAs in 'temp' and 'cisnienie' variables
dat <- data1 %>%
  dplyr::filter(!is.na(data1$temp))

write.table(dat, file = "learning_curve_data.txt", sep = ";", row.names = FALSE)


```

```{r}

##################################################################
#
#       Modification of an algoritm from the R-bloggers
#    https://www.r-bloggers.com/why-big-data-learning-curves/
#
##################################################################

library(data.table)
library(caret)
library(ggplot2)
library(dplyr)
library(doParallel)

# We will be using RMSE score
rmse <- function(actual, predicted) sqrt( mean( (actual - predicted)^2 ))

run_learning_curve <- function(predictors,outcome,vss=200, num_tss=10, min_tss=100, method = "lm"){
  # predictors - data frame contains predictors(independent variables, "x")
  # outcome - a vector of dependent variable("y")
  # vss - the validation set size 
  # tss - the number of different training set sizes we want to plot 
  # num_tss - the smallest training set size to start with
  
  max_tss <- nrow(predictors) - vss
  tss_vector <- seq(min_tss, max_tss, length=num_tss)

  data.table::rbindlist( lapply (tss_vector, function(tss){
    vs_idx <- sample(1:nrow(predictors), vss)             #as for me - this should be taken outside of the inner function
    ts_eligible <- setdiff(1:nrow(predictors), vs_idx)
    ts <- sample(ts_eligible, tss)
    fit <- train(predictors[ts,], outcome[ts],method = method)
   
    training_error <- rmse(outcome[ts], predict(fit, predictors[ts,]))  
    validation_error <- rmse(outcome[vs_idx], predict(fit, predictors[vs_idx,]))
    
    data.frame(tss=tss, 
                error_type = factor(c("training", "validation"), 
                                    levels=c("validation", "training")),
                error=c(training_error, validation_error))
   
  }) )
}


cl<-makeCluster(3)
registerDoParallel(cl)

learning_curve <- run_learning_curve(dplyr::select(dat, -traffic),dat$traffic, num_tss=10)

stopCluster(cl) 
ggplot(learning_curve, aes(x=tss, y=error, linetype=error_type)) + 
  geom_line(size=1, col="blue") + xlab("training set size") + geom_hline(yintercept=2000, linetype=3)


```

```{r}
##################################################################
#
#
#           We add averaging by cross validation sets 
#       using k-fold split to smooth outcoming plot and not to 
#   get affected by arbitrary chosen only one cross validation set
#
# 
# 
#  A cross-validation generator splits the whole dataset k times in training and test data.
#  Subsets of the training set with varying sizes will be used to train the estimator and a score 
#  for each training subset size and the test set will be computed. 
#  Afterwards, the scores will be averaged over all k runs for each training subset size.
#
#       Modification of an algoritm from the R-bloggers
#    https://www.r-bloggers.com/why-big-data-learning-curves/
#
##################################################################

library(data.table)
library(caret)
library(ggplot2)
library(dplyr)
library(doParallel)


# We will be using RMSE score
rmse <- function(actual, predicted) sqrt( mean( (actual - predicted)^2 ))

run_learning_curve <- function(predictors,outcome, num_tss=10, min_tss=100, method = "lm", cv_split = 3){
  # predictors - data frame containing predictors(independent variables, "x")
  # outcome - a vector of dependent variable("y")
  # cv_split - number of folds(validation sets = test sets) we split the dataset into. 
  #            The final outcome of the learning curve is an average of the calculated 
  #            errors (cv and test) for each validation set(fold) 
  # num_tss - the number of different training set sizes we divide the whole training 
  #           set into, the number of x-axis ticks in learning curve plot
  # min_tss - the smallest training set size to start with
  # method - CARET model method   
  
  # split input set into cross validation sets  
  cv_folds <- createFolds(1:nrow(predictors), k = cv_split)
  
  # output vector of errors (training and validation - hence it needs to be multiplied by 2)  
  sum_errors <- rep(0, 2*num_tss) 
  
   for (fold in cv_folds){
     # use cross validation set to calculate cross validation set errors for each model trained on trainings sets 
     # and calcualte corresponding training set errors for each training set
     max_tss <- nrow(predictors) - length(fold)
     tss_vector <- seq(min_tss, max_tss, length=num_tss)
     # vs_idx <- fold
     
     error_dataframe <- data.table::rbindlist( lapply (tss_vector, function(tss){
           ts_eligible <- setdiff(1:nrow(predictors), fold)
           ts <- sample(ts_eligible, tss)
           
           fit <- train(predictors[ts,], outcome[ts],method = method)
           training_error <- rmse(outcome[ts], predict(fit, predictors[ts,]))  
           validation_error <- rmse(outcome[fold], predict(fit, predictors[fold,]))
                  
           data.frame(tss=tss, 
                       error_type = factor(c("training", "validation"), 
                                           levels=c("validation", "training")),
                       error=c(training_error, validation_error))
           } 
        ))
      
      sum_errors <- sum_errors+error_dataframe[,3]  
   }
   
  averaged_error_dataframe =  data.frame(error_dataframe[,-3],error = sum_errors/cv_split)
  return(averaged_error_dataframe)
}


# calculate learning curve
cl<-makeCluster(3)
registerDoParallel(cl)
learning_curve <- run_learning_curve(dplyr::select(dat, -traffic),dat$traffic, num_tss=10,cv_split = 5)
stopCluster(cl) 

# Plot learning curve
ggplot(learning_curve, aes(x=tss, y = error, linetype = error_type)) + 
  geom_line(size=1, col="blue") + xlab("training set size") + geom_hline(yintercept=2000, linetype=3)





```


```{r}

##########################################################################
# Random forest - with modificated run_learning_curve function
##########################################################################

library(randomForest)

cl<-makeCluster(3)
registerDoParallel(cl)
start.time <- Sys.time()

learning_curve <- run_learning_curve(dplyr::select(dat, -traffic),dat$traffic, num_tss=8, min_tss=200, cv_split = 5, method = "rf")

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
stopCluster(cl) 

ggplot(learning_curve, aes(x=tss, y=error, linetype=error_type)) + 
  geom_line(size=1, col="blue") + xlab("training set size") + geom_hline(yintercept=1000, linetype=3)
```

```{r}
##########################################################################
# Random forest - with modificated run_learning_curve function
##########################################################################

library(Cubist)

cl<-makeCluster(3)
registerDoParallel(cl)
start.time <- Sys.time()

learning_curve <- run_learning_curve(dplyr::select(dat, -traffic),dat$traffic, num_tss=8, min_tss=200, cv_split = 5, method = "cubist")

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
stopCluster(cl) 

ggplot(learning_curve, aes(x=tss, y=error, linetype=error_type)) + 
  geom_line(size=1, col="blue") + xlab("training set size") + geom_hline(yintercept=1000, linetype=3)
```

```{python}

# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.model_selection import learning_curve




data = pd.read_csv("C:\\Users\\jarek\\Documents\\GitHub\\Learning_Curves\\learning_curve_data.txt", sep = ';')




X = data.drop(data.columns[[4]],axis = 1)
y = data['traffic']



```

```{python}

%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.model_selection import learning_curve




data = pd.read_csv("C:\\Users\\jarek\\Documents\\GitHub\\Learning_Curves\\learning_curve_data.txt", sep = ';')




X = data.drop(data.columns[[4]],axis = 1)
y = data['traffic']

import numpy as np
import matplotlib.pyplot as plt

def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):
    """
    Generate a simple plot of the test and training learning curve.

    Parameters
    ----------
    estimator : object type that implements the "fit" and "predict" methods
        An object of that type which is cloned for each validation.

    title : string
        Title for the chart.

    X : array-like, shape (n_samples, n_features)
        Training vector, where n_samples is the number of samples and
        n_features is the number of features.

    y : array-like, shape (n_samples) or (n_samples, n_features), optional
        Target relative to X for classification or regression;
        None for unsupervised learning.

    ylim : tuple, shape (ymin, ymax), optional
        Defines minimum and maximum yvalues plotted.

    cv : int, cross-validation generator or an iterable, optional
        Determines the cross-validation splitting strategy.
        Possible inputs for cv are:
          - None, to use the default 3-fold cross-validation,
          - integer, to specify the number of folds.
          - An object to be used as a cross-validation generator.
          - An iterable yielding train/test splits.

        For integer/None inputs, if ``y`` is binary or multiclass,
        :class:`StratifiedKFold` used. If the estimator is not a classifier
        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.

        Refer :ref:`User Guide <cross_validation>` for the various
        cross-validators that can be used here.

    n_jobs : integer, optional
        Number of jobs to run in parallel (default 1).
    """
    plt.figure()
    plt.title(title)
    if ylim is not None:
        plt.ylim(*ylim)
    plt.xlabel("Training examples")
    plt.ylabel("Score")
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    plt.grid()

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1,
                     color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
             label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
             label="Cross-validation score")

    plt.legend(loc="best")
    return plt
    
    
from sklearn import linear_model
import sklearn
# Create linear regression object
lm = linear_model.LinearRegression()

title = "Learning Curve lm"

plot_learning_curve(lm, title, X, y, ylim=None, cv=10,train_sizes=np.linspace(.2, 1.0, 10))   
    
    
    
    
    
```


```{python}

from sklearn import linear_model
import sklearn
# Create linear regression object
lm = linear_model.LinearRegression()

title = "Learning Curve lm"

plot_learning_curve(lm, title, X, y, ylim=None, cv=10,train_sizes=np.linspace(.2, 1.0, 10))



```

```{python}
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(max_depth=2, random_state = 0)

title = "Learning Curve Random Forest"

plot_learning_curve(rf, title, X, y, ylim=None, cv=10,train_sizes=np.linspace(.2, 1.0, 10))

```

